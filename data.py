# -*- coding: utf-8 -*-
"""data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QmQQFrC45zZ-q981MHrZfwKJb4yaGA_t

# **OuedKniss Data-scraping**
"""

import requests
import csv
import re
from bs4 import BeautifulSoup
import pandas as pd
import time

def extract_details(description):
    details = {
        "Contract Type": "Not specified",
        "Skills": [],
        "Responsibilities": []
    }

    # Check if the description is None or empty
    if not description:
        return details

    # Clean HTML if present
    soup = BeautifulSoup(description, 'html.parser')
    cleaned_text = soup.get_text(separator="\n")

    # Split the text into lines
    lines = cleaned_text.split("\n")

    # Initialize flags to determine which section we're in
    in_skills_section = False
    in_responsibilities_section = False

    for line in lines:
        line = line.strip()

        # Detect and handle the "Responsibilities" section
        if line.lower().startswith(("responsibilities", "responsabilités","tâches","missions")):
            in_skills_section = False
            in_responsibilities_section = True
            continue

        # Detect and handle the "Skills" section
        elif line.lower().startswith(("skills", "compétences","exigences","profil")):
            in_skills_section = True
            in_responsibilities_section = False
            continue

        # Extract contract type, if mentioned directly
        elif "contract" in line.lower() or "contrat" in line.lower() or "type de contrat" in line.lower() or "type de contat" in line.lower():
            details["Contract Type"] = line.split(":")[-1].strip()

        # Add lines to skills or responsibilities based on the current section
        if in_skills_section:
            if line:
                details["Skills"].append(line)
        elif in_responsibilities_section:
            if line:
                details["Responsibilities"].append(line)


    return details

def main():
    url = "https://api.ouedkniss.com/graphql"
    headers = {
        "authorization": "Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJpc3MiOiJodHRwOi8vYXV0aC9sb2dpbi9zb2NpYWwiLCJpYXQiOjE3MjQ0NDM3NDEsImV4cCI6MTcyNTczOTc0MSwibmJmIjoxNzI0NDQzNzQxLCJqdGkiOiJydDhFTEFKY2dZSDBuSGRZIiwic3ViIjoiMTE1MDA5OTIiLCJwcnYiOiIyM2JkNWM4OTQ5ZjYwMGFkYjM5ZTcwMWM0MDA4NzJkYjdhNTk3NmY3IiwidXNlciI6eyJpZCI6MTE1MDA5OTIsInVzZXJuYW1lIjoiOGNianl0bzFndjA4MTIiLCJpc19hbm9ueW1vdXMiOjAsInJvbGVzIjpbIk1FTUJFUiIsIk1FU1NBR0lORyJdLCJwZXJtaXNzaW9ucyI6WyJjYW5fcmVjZWl2ZV9tZXNzYWdlIiwiY2FuX2NyZWF0ZV90aHJlYWQiLCJjYW5fbGlzdF90aHJlYWQiXX19.BoqGdGIKAfWl1iVrQuyBl0dVeT-QlGDJzKI_P5wX6yHK4WafAWGFHy3ZgyAIuhcf3sFTKPa2452VKSWiEJ8qSvpK7OimsyMedhTrUnZeWRaDf3c6vxuVRqEPiE5YMDiJPB2ZaQh5Mc8R8phemIRtRC4GhmyCQR3yU3_Yk1dsC7JuBWGsq9Y3poTTWn9v4V0xlXqS7xIVl4vDsvRxx9hxSTZ6ptfAhye6BXsMf4lxcIlISsawNnfZvKfctkTHeZelB0tAotpgP8BENCbq7fQjTt1pVfN3_3oHBF0ceJwtcquYwOzIsVthOOMwxgisMedLlJ1UqcT8MYIcGAh4XC0tHg",
        "content-type": "application/json",
        "x-app-version": "3.0.40",
        "referer": "https://www.ouedkniss.com/emploi_offres-informatique-internet/1",
        "origin": "https://www.ouedkniss.com",
        "user-agent": "Mozilla/5.0"
    }

    all_announcements = []
    page = 1
    has_more_pages = True

    while has_more_pages:
        time.sleep(1)
        payload = {
            "operationName": "SearchQuery",
            "variables": {
                "mediaSize": "MEDIUM",
                "q": None,
                "filter": {
                    "categorySlug": "emploi_offres-informatique-internet",
                    "origin": None,
                    "connected": False,
                    "delivery": None,
                    "regionIds": [],
                    "cityIds": [],
                    "priceRange": [None, None],
                    "exchange": False,
                    "hasPictures": False,
                    "hasPrice": False,
                    "priceUnit": None,
                    "fields": [],
                    "page": page,
                    "count": 48
                }
            },
            "query": """
            query SearchQuery($q: String, $filter: SearchFilterInput, $mediaSize: MediaSize = MEDIUM) {
                search(q: $q, filter: $filter) {
                    announcements {
                        data {
                            id
                            title
                            description
                            price
                            createdAt
                            cities {
                                name
                            }
                            defaultMedia(size: $mediaSize) {
                                mediaUrl
                            }
                        }
                        paginatorInfo {
                            lastPage
                            hasMorePages
                        }
                    }
                }
            }
            """
        }

        response = requests.post(url, json=payload, headers=headers)

        if response.status_code == 200:
            data = response.json()
            announcements = data['data']['search']['announcements']['data']
            all_announcements.extend(announcements)

            has_more_pages = data['data']['search']['announcements']['paginatorInfo']['hasMorePages']
            print(f"Page {page} fetched. More pages? {has_more_pages}")
            page += 1
        else:
            print(f"Request error: {response.status_code}")
            print(response.text)
            break

    flattened_data = []
    for announcement in all_announcements:
        description = announcement.get('description', '')


        details = extract_details(description)

        flattened_data.append({
            "ID": announcement.get('id'),
            "Title": announcement.get('title'),
            "Description": description,
            "Price": announcement.get('price'),
            "Created At": announcement.get('createdAt'),
            "City": announcement['cities'][0]['name'] if announcement.get('cities') else '',
            "Contract Type": details["Contract Type"],
            "Skills": ', '.join(details["Skills"]),
            "Responsibilities": ', '.join(details["Responsibilities"])
        })

    df = pd.DataFrame(flattened_data)
    df.to_csv('ouedkniss_all_announcements_detailed.csv', index=False)
    print("All data saved to ouedkniss_all_announcements_detailed.csv")

if __name__ == "__main__":
    main()

"""# **EmploiPartner Data-Scraping**"""




import csv
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options

from bs4 import BeautifulSoup
import requests
import time

# Set up Chrome options for headless mode
chrome_options = Options()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
chrome_options.add_argument('--disable-gpu')
chrome_options.add_argument('--remote-debugging-port=9222')

# Initialize the WebDriver with options
driver = webdriver.Chrome(options=chrome_options)

# URL to scrape
url = 'https://www.emploipartner.com/fr/offre-emploi'

# Open the webpage
driver.get(url)
time.sleep(5)  # Allow time for the page to load
# Scroll to load all job offers
last_height = driver.execute_script("return document.body.scrollHeight")
while True:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(3)  # Allow time for content to load
    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height

job_links = []
job_data = []
with open('job_offers_EmploiPartner3.csv', mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['Title', 'Location', 'Description', 'Date', 'Contract Type'])

    # job offers
    job_offers_container = driver.find_element(By.ID, 'ep_job_list')
    job_offer_divs = job_offers_container.find_elements(By.CLASS_NAME, 'col-xs-12.margin-top-10')
    for job_offer_div in job_offer_divs:
        try:
            #  job title
            title_tag = job_offer_div.find_element(By.TAG_NAME, 'h3')
            title = title_tag.text.strip() if title_tag else "No title"

            #  job location
            location_tag = job_offer_div.find_element(By.CSS_SELECTOR, 'p.logo-location.hidden-xs')
            location = location_tag.text.strip() if location_tag else "No location"

            #job description
            description_tag = job_offer_div.find_element(By.CSS_SELECTOR, 'p.hide-on-mobile span')
            description = description_tag.text.strip() if description_tag else "No description"

            #  job date
            date_tag = job_offer_div.find_element(By.CSS_SELECTOR, 'div.col-xs-8.col-md-9 span')
            date = date_tag.text.strip() if date_tag else "No date"

            # contract type
            contract_type_tag = job_offer_div.find_element(By.CSS_SELECTOR, 'div.col-xs-4.col-md-3 h5.contract-type')
            contract_type = contract_type_tag.text.strip() if contract_type_tag else "No contract type"
            #link
            link = job_offer_div.find_element(By.CSS_SELECTOR, 'h3.function-title a').get_attribute('href')
            job_links.append(link)

            job_data.append([title, location, description, date, contract_type, link])

            #  CSV file
            writer.writerow([title, location, description, date, contract_type])

        except Exception as e:
            print(f"Error extracting job offer details: {e}")
       # Close the WebDriver
driver.quit()

# Open CSV file for writing
with open('job_offers_EmploiPartner3.csv', mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['Title', 'Location', 'Description', 'Date', 'Contract Type', 'Missions', 'Profil'])

    # Loop through all job links and parse details using BeautifulSoup
    for job in job_data:
        title, location, description, date, contract_type, link = job
        try:
            response = requests.get(link)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')

                description_tag = soup.find('div', class_='job-description')
                description = description_tag.text.strip() if description_tag else "No description"

                # Extract missions
                missions_section = soup.find('h3', class_='offer-title', string='Missions')
                missions = missions_section.find_next('ul').text.strip() if missions_section else "No missions"

                # Extract profil
                profil_section = soup.find('h3', class_='offer-title', string='Profil')
                profil = profil_section.find_next('ul').text.strip() if profil_section else "No profil"

                # Write to CSV
                writer.writerow([title, location, description, date, contract_type, missions, profil])

            else:
                print(f"Failed to retrieve {link}: Status code {response.status_code}")

        except Exception as e:
            print(f"Error processing job link {link}: {e}")



print("Scraping completed and data written to CSV.")

"""# **Data Pre-Processing**"""

import pandas as pd

# Load the CSV files
emploipartner_df = pd.read_csv('job_offers_EmploiPartner3.csv')
ouedkniss_df = pd.read_csv('ouedkniss_offers.csv')

# Rename columns to make them consistent
ouedkniss_df.rename(columns={
    'Created At': 'Date',
    'City': 'Location'
}, inplace=True)
# Rename columns to make them consistent
emploipartner_df.rename(columns={
    'Missions': 'Responsibilities',
    'Profil': 'Skills'
}, inplace=True)

# Drop the 'ID' column
ouedkniss_df.drop(columns=['ID'], inplace=True)

# Combine them into one DataFrame
combined_df = pd.concat([emploipartner_df, ouedkniss_df], ignore_index=True)

# Save the combined data into a new CSV file
combined_df.to_csv('combined_cleaned_job_offers.csv', index=False)

print("Data combined and cleaned successfully!")

combined_df['Price'] = combined_df['Price'].astype(str)
combined_df.fillna('nv', inplace=True)
combined_df.to_csv('combined_cleaned_job_offers.csv', index=False)

# Remove duplicates based on all columns
combined_df.drop_duplicates(inplace=True)

# Save the de-duplicated data
combined_df.to_csv('combined_cleaned_job_offers_no_duplicates.csv', index=False)

print("Duplicates removed successfully!")

df = pd.read_csv('combined_cleaned_job_offers_no_duplicates.csv')



"""# **Translate to English**"""

from googletrans import Translator

translator = Translator()

def translate_to_english(text):
  if text is not None and text != 'nv':
    return translator.translate(text, dest='en').text
  return text

# Apply the translation function to relevant columns
df['Title'] = df['Title'].apply(translate_to_english)
df['Description'] = df['Description'].apply(translate_to_english)
df['Location'] = df['Location'].apply(translate_to_english)
df['Contract Type'] = df['Contract Type'].apply(translate_to_english)
df['Responsibilities'] = df['Responsibilities'].apply(translate_to_english)
df['Skills'] = df['Skills'].apply(translate_to_english)

# Save the translated DataFrame to a new CSV file
df.to_csv('translated_job_offers.csv', index=False)

print("Translation completed and saved to 'translated_job_offers.csv'.")

"""# **System of Courses recomendation**"""



import os
from google.colab import userdata
import csv
import openai

# Set up environment variables
os.environ["GROQ_API_KEY"] = userdata.get('GROQ_API_KEY')
os.environ["OPENAI_API_KEY"] = os.environ.get("GROQ_API_KEY")
os.environ["OPENAI_BASE_URL"] = "https://api.groq.com/openai/v1"

# Configure OpenAI with Groq settings
openai.api_key = os.environ["OPENAI_API_KEY"]
openai.api_base = os.environ["OPENAI_BASE_URL"]

# Define the model name
MODEL_NAME = "llama-3.1-70b-versatile"

import csv
import openai

# Set up your OpenAI (or Groq) API key if needed
# openai.api_key = "your_api_key"

def recommend_courses(skills):
    prompt = f"Given the following skills: {skills}, recommend a list of suitable course titles."
    response = openai.ChatCompletion.create(
        model=MODEL_NAME,
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response['choices'][0]['message']['content'].strip().split('\n')

example_skills = "Python, Machine Learning, Data Analysis"
recommended_courses = recommend_courses(example_skills)
print("Recommended Courses for Example Skills:", recommended_courses)

# Load your CSV file
csv_file = 'job_offers.csv'  # Replace with your actual CSV file path
output_file = 'recommended_courses.csv'  # Output file for recommendations

with open(csv_file, mode='r', encoding='utf-8') as file, open(output_file, mode='w', newline='', encoding='utf-8') as output:
    reader = csv.DictReader(file)
    fieldnames = reader.fieldnames + ['Recommended Courses']  # Add a new column for recommendations
    writer = csv.DictWriter(output, fieldnames=fieldnames)
    writer.writeheader()

    for row in reader:
        skills = row['Skills']  # Get the skills from the row
        print(f"Processing skills: {skills}")  # Print the skills for debugging
        try:
            recommended_courses = recommend_courses(skills)  # Generate recommendations
            row['Recommended Courses'] = ', '.join(recommended_courses)  # Join the recommendations into a single string
        except Exception as e:
            row['Recommended Courses'] = f"Error: {e}"  # Handle any errors from the API
        writer.writerow(row)

print(f"Course recommendations have been saved to {output_file}")

"""**Testing only 20 first jobs**"""

csv_file = 'job_offers.csv'  # Replace with your actual CSV file path
output_file = 'recommended_courses.csv'  # Output file for recommendations

with open(csv_file, mode='r', encoding='utf-8') as file, open(output_file, mode='w', newline='', encoding='utf-8') as output:
    reader = csv.DictReader(file)
    fieldnames = reader.fieldnames + ['Recommended Courses']  # Add a new column for recommendations
    writer = csv.DictWriter(output, fieldnames=fieldnames)
    writer.writeheader()

    counter = 0  # Initialize counter

    for row in reader:
        if counter >= 20:  # Stop after processing 20 rows
            break

        skills = row['Skills']  # Get the skills from the row
        print(f"Processing skills: {skills}")  # Print the skills for debugging
        try:
            recommended_courses = recommend_courses(skills)  # Generate recommendations
            row['Recommended Courses'] = ', '.join(recommended_courses)  # Join the recommendations into a single string
        except Exception as e:
            row['Recommended Courses'] = f"Error: {e}"  # Handle any errors from the API
        writer.writerow(row)
        counter += 1  # Increment counter

print(f"Course recommendations have been saved to {output_file}")